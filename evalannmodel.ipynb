{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da93df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np; # Mostly used for array manipulation datatype numpy.ndarray\n",
    "from sklearn.linear_model import LinearRegression # Used for implementing Logistic regression\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt # Plotting in python3 using Matplotlib library\n",
    "import math # math module for exp function; For e value\n",
    "# generate random integer values\n",
    "from numpy.random import seed\n",
    "from numpy.random import randint\n",
    "import array\n",
    "import random\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "import csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c01286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_shapes(num_oflayers,nodes_eachlayer,rows,cols,eachlayer,shape_wts,shape_b,shape_error):\n",
    "    for eachlayer in range(num_oflayers):\n",
    "        ## shape of weights in given current layer is nodesinpreviouslayer X nodesincurrentlayer\n",
    "        if(eachlayer == 0):\n",
    "            shape_forwt = (cols,nodes_eachlayer[1])\n",
    "            shape_wts[str(eachlayer)] = shape_forwt\n",
    "            \n",
    "            shape_forb = cols\n",
    "            shape_b[str(eachlayer)] = shape_forb \n",
    "            \n",
    "            shape_forerror = rows\n",
    "            shape_error[str(eachlayer)] = shape_forerror\n",
    "            \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            shape_forwt = (nodes_eachlayer[eachlayer-1],nodes_eachlayer[eachlayer])\n",
    "            shape_wts[str(eachlayer)] = shape_forwt\n",
    "                        \n",
    "            shape_forb = nodes_eachlayer[eachlayer]\n",
    "            shape_b[str(eachlayer)] = shape_forb\n",
    "            \n",
    "            shape_forerror = rows\n",
    "            shape_error[str(eachlayer)] = shape_forerror\n",
    "    \n",
    "    \n",
    "    return shape_wts,shape_b,shape_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeandsavedata(headers,data,flag):\n",
    "    filenames = ['original','train','test','validation']\n",
    "    features = data\n",
    "    if(flag == 0): # Entire data into one single file\n",
    "        with open(filenames[0], 'w') as csvfile:\n",
    "            mywriter = csv.writer(csvfile, delimiter=',')\n",
    "            mywriter.writerow(headers)\n",
    "            mywriter.writerows(features)\n",
    "            csvfile.close()\n",
    "        #outfilename = filenames[0] + 'outputlabels'     \n",
    "        \n",
    "            \n",
    "            \n",
    "    elif(flag == 1): # Entire data into train file\n",
    "        with open(filenames[1], 'w') as csvfile:\n",
    "            mywriter = csv.writer(csvfile, delimiter=',')\n",
    "            mywriter.writerow(headers)\n",
    "            mywriter.writerows(features)\n",
    "            csvfile.close()\n",
    "        #outfilename = filenames[1] + 'outputlabels'         \n",
    "        \n",
    "            \n",
    "    elif(flag == 2): # Entire data into test file\n",
    "        with open(filenames[2], 'w') as csvfile:\n",
    "            mywriter = csv.writer(csvfile, delimiter=',')\n",
    "            mywriter.writerow(headers)\n",
    "            mywriter.writerows(features)\n",
    "            csvfile.close()\n",
    "        #outfilename = filenames[2] + 'outputlabels'         \n",
    "            \n",
    "    \n",
    "    else:\n",
    "        with open(filenames[3], 'w') as csvfile:\n",
    "            mywriter = csv.writer(csvfile, delimiter=',')\n",
    "            mywriter.writerow(headers)\n",
    "            mywriter.writerows(features)\n",
    "            csvfile.close()\n",
    "        #outfilename = filenames[3] + 'outputlabels'     \n",
    "        \n",
    "    #with open(outfilename, 'w') as csvfile:\n",
    "    #        mywriter = csv.writer(csvfile)\n",
    "    #        mywriter.writerow('diagnosis')\n",
    "    #        mywriter.writerow('\\n')\n",
    "    #        mywriter = csv.writer(csvfile, delimiter=',')\n",
    "    #        for eachrow in range(len(outputlabels)):\n",
    "    #            mywriter.writerow(outputlabels[eachrow])\n",
    "    #            mywriter.writerow('\\n')\n",
    "    #        csvfile.close()\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatewandb(learningratew,learningrateb,weights,b,dj_dw,dj_db):\n",
    "    weights = weights-learningratew * dj_dw\n",
    "    b = b-learningrateb * dj_db\n",
    "    \n",
    "    return weights,b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ec027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computecostfunction(lossfunction,rows,weights,reg_term):\n",
    "      \n",
    "    summationterm = 0\n",
    "    for eachweight in weights: # weights is of order: numoffeatures X numofnodesinhiddenlayer\n",
    "        #print(eachweight,'\\n')\n",
    "        summationterm += sum((eachweight**2))\n",
    "    costfunction = sum(lossfunction)/(2*rows) + (reg_term/2*rows)*summationterm\n",
    "    \n",
    "    return costfunction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computedjdw(dj_dw,dj_db,error,features,rows,cols):\n",
    "         \n",
    "    for eachrow in range(rows):\n",
    "        for eachcol in range(cols):\n",
    "            dj_dw[eachcol] = dj_dw[eachcol] + error[eachrow] + features[eachrow,eachcol]\n",
    "        dj_db = dj_db + error[eachrow]\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg(inputs_activations):\n",
    "    #print(inputs_activations)\n",
    "    #inputs_activations = np.clip(inputs_activations,a_min = 0, a_max = 1)\n",
    "    #print(inputs_activations)\n",
    "    log_reg_output = 1/(1+np.exp(-inputs_activations))\n",
    "    \n",
    "    #print(\"shape of logistic reg output:\",np.shape(log_reg_output))\n",
    "    #print(\"\\n\")\n",
    "    #print(\"log regression outputs, BEFORE clipping:\\n\")\n",
    "    #print(log_reg_output)\n",
    "    #print(\"\\n\")\n",
    "    #log_reg_output = np.clip(log_reg_output,a_min = 1e-5, a_max = 0.55)\n",
    "    #print(\"log regression outputs, AFTER clipping:\\n\")\n",
    "    #print(log_reg_output)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return log_reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(features,weights,b):\n",
    "    lin_reg_output = np.dot(features,weights)+b\n",
    "    #print(\"Before clipping:\",lin_reg_output)\n",
    "    #lin_reg_output = np.clip(lin_reg_output,a_min = 500, a_max = 501)\n",
    "    \n",
    "    #print(\"shape of input activations:\",np.shape(features))\n",
    "    #print(\"shape of weights:\",np.shape(weights))\n",
    "    #print(\"shape of b:\",np.shape(b))\n",
    "    #print(\"shape of lin reg output:\",np.shape(lin_reg_output))\n",
    "    #print(\"lin reg output:\",lin_reg_output,'\\n')\n",
    "    return lin_reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb144d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(inputs_activations,rows,cols,diagnosis_res,error,lossfunction):\n",
    "    # error computed from subtracting logistic output & diagnosis \n",
    "    #log_reg_output = logistic_reg(inputs_activations)\n",
    "    epsilon = 0 #1e-2\n",
    "    output = inputs_activations\n",
    "    for eachrow in range(rows):\n",
    "        error[eachrow] = abs(output[eachrow] - diagnosis_res[eachrow])\n",
    "        #print(\"\\n \\n \\n \")\n",
    "        #print(\"loss function calculation:\")\n",
    "        \n",
    "        #print(\"value of nplog:\",np.log(output[eachrow]))\n",
    "        #print(\"value of diagnosis:\",diagnosis_res[eachrow])\n",
    "        #output[eachrow] = np.clip(output[eachrow],a_min = 1e-2, a_max = 0.999)\n",
    "        lossfunction[eachrow] = (-(diagnosis_res[eachrow])*np.log(output[eachrow]+epsilon))-(1-diagnosis_res[eachrow])*(np.log(1-output[eachrow]+epsilon))\n",
    "        #print(\"value of lossfunction:\",lossfunction[eachrow])\n",
    "        \n",
    "    return error,lossfunction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "       \n",
    "    # Input feature 1 patient age\n",
    "    # Input feature 2 tumor size\n",
    "    # Input feature 3 family history (y/n:0/1)\n",
    "    # Input feature 4 regular junk food intake\n",
    "    # Input feature 5 currently smoking\n",
    "    # Input feature 6 stressful lifestyle\n",
    "    # Input feature 7 working with chemicals\n",
    "    # Input feature 8 urban/rural (0/1)\n",
    "    # Input feature 9 sleep duration (1hr-10hrs)\n",
    "    # Input feature 10 regular exercise\n",
    "    \n",
    "    num_ofpatients = 398\n",
    "    pat_age = randint(0,100,num_ofpatients)\n",
    "    tum_size = randint(1,10,num_ofpatients) # in cm\n",
    "    fm_histry = randint(0,2,num_ofpatients)\n",
    "    junk_food = randint(0,2,num_ofpatients)\n",
    "    smoking = randint(0,2,num_ofpatients)\n",
    "    stress_lif = randint(0,2,num_ofpatients)\n",
    "    work_withchem = randint(0,2,num_ofpatients)\n",
    "    rural_urbn = randint(0,2,num_ofpatients)\n",
    "    sleep_dur = randint(1,10,num_ofpatients)\n",
    "    reg_exerc = randint(0,2,num_ofpatients)\n",
    "    features = np.transpose([pat_age, tum_size, fm_histry, junk_food,smoking,stress_lif,work_withchem,rural_urbn,sleep_dur,reg_exerc])\n",
    "    features = np.transpose([pat_age, tum_size, fm_histry, junk_food,smoking,stress_lif,work_withchem,rural_urbn,sleep_dur,reg_exerc])\n",
    "    \n",
    "    \n",
    "    [rows,cols] = np.shape(features)\n",
    "    \n",
    "    headers = ['pat_age','tum_size','fm_history','junk_food','smoking','stress_lif','work_withchem','rural_urbn','sleep_dur','reg_exerc']\n",
    "    \n",
    "    \n",
    "    # Output benign or malignant 0/1\n",
    "    diagnosis_res = randint(0,2,num_ofpatients)\n",
    "    #print(\"\\n \\n\")\n",
    "    #print(\"Diagnosis values:\")\n",
    "    #print(diagnosis_res)\n",
    "    #print(\"\\n \\n\")\n",
    "    \n",
    "    \n",
    "    #################   Store entire original dataset in CV format ################\n",
    "    ##################           (Only Input data)                 ###############  \n",
    "    ###################                                             ##############\n",
    "    filename = '/home/sreeharish/Desktop/MLspecialization/AdvancedlearningAlgorithms/Week3/patientdata.csv'\n",
    "    \n",
    "    # For original data 0 to rows\n",
    "    original_data_indices = np.arange(int(0),int(rows))\n",
    "    #print(original_data_indices)\n",
    "    # For Train data take half of total_datarows\n",
    "    train_data_indices = np.arange(0,int(rows/2))\n",
    "        \n",
    "    # For Test data traindataend+1 row to 1/4th of total data \n",
    "    test_data_indices = np.arange(int(rows/2),int(rows/2)+1+int(rows/4))\n",
    "        \n",
    "        \n",
    "    #print(int(rows/2)+1+int(rows/4)+1,'\\n')\n",
    "    #print(int(rows/2)+1+int(rows/4)+1+int(rows/4))\n",
    "    # For validation data Testdataend+1 row to 1/4th of total data\n",
    "    validation_data_indices = np.arange(int(rows/2)+1+int(rows/4),int(rows/2)+1+int(rows/4)+1+int(rows/4)-1)\n",
    "    \n",
    "    \n",
    "    indices_dict = {0:original_data_indices,1:train_data_indices,2:test_data_indices,3:validation_data_indices}\n",
    "    for key in indices_dict:\n",
    "        #print(\"Key\",key)\n",
    "        flag = key      \n",
    "        #print(\"Data indices:\",indices_dict[flag],\"\\n\")\n",
    "        data = features[indices_dict[flag]]   \n",
    "        #outputlabels = diagnosis_res[indices_dict[flag]]\n",
    "        #print(\"Output labels sHAPE:\",np.shape(outputlabels),\"\\n\")\n",
    "        writeandsavedata(headers,data,flag)\n",
    "    \n",
    "   \n",
    "    #### Successfully split original data into test, train and validation data ###########\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Train ANN ###########\n",
    "    \n",
    "    # Step 1) Prepare ANN architecture\n",
    "    print(\"Enter number of layers:\")\n",
    "    num_oflayers = int(input())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Prepare each layer with desired number of nodes ######\n",
    "    \n",
    "    nodes_eachlayer = np.empty([num_oflayers],dtype=int)\n",
    "    \n",
    "    \n",
    "    for eachlayer in range(num_oflayers):\n",
    "       \n",
    "        if(eachlayer == 0):\n",
    "            nodes_eachlayer[eachlayer] = cols\n",
    "        else:\n",
    "            print(\"Enter num of nodes for layer no:\", eachlayer)\n",
    "            nodes_eachlayer[eachlayer] = int(input())\n",
    "    \n",
    "    \n",
    "    \n",
    "    ######## Prepare shapes of weights and bias per each layer #########\n",
    "    dataset_names = ['original','train','test','validation']\n",
    "    for eachdataset in range(4):\n",
    "        if(eachdataset > 0):\n",
    "            weights = {}\n",
    "            b = {}\n",
    "            shape_wts = {}\n",
    "            shape_b = {}\n",
    "            shape_error = {}\n",
    "            lossfunction = {}\n",
    "            [rows, cols] = np.shape(features[indices_dict[eachdataset]])\n",
    "            #print(\"For {} dataset\".format(dataset_names[eachdataset]),\"\\n\")\n",
    "            #print(\"Dataset size:{} X {}\".format(rows,cols),\"\\n\")\n",
    "            \n",
    "            \n",
    "            shape_wts, shape_b, shape_error = prepare_shapes(num_oflayers,nodes_eachlayer,rows,cols,eachlayer,shape_wts,shape_b,shape_error)\n",
    "    \n",
    "            #print(\"weight shapes dictionary\",shape_wts)\n",
    "            #print(\"b shapes dictionary\",shape_b)\n",
    "            #print(\"Error shapes dictionary\",shape_error)\n",
    "            #print(\"\\n\")    \n",
    "    \n",
    "    ############# No ambiguity w.r.t shapes of weights and bias for each layer ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    error = {} \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    #####################   Shape of weights and biases same for training, test  ###############\n",
    "    ######################             and validation sets                         ############\n",
    "    #########################   as they are dependant only on number of nodes #########\n",
    "    #########################         in hidden layer and not data shape       ##########                                                  ##############\n",
    "    \n",
    "    \n",
    "    # initialize weights and b before running ANN, which go into 1st epoch of the ANN\n",
    "    \n",
    "    for eachlayer in range(num_oflayers):\n",
    "        [rows, cols] = np.shape(features)\n",
    "        # [numofnodespreviouslayer,numofnodescurrlayer] = \n",
    "        numofnodespreviouslayer = shape_wts[str(eachlayer)][0]\n",
    "        numofnodescurrlayer = shape_wts[str(eachlayer)][1]\n",
    "        weights[str(eachlayer)] = randint(1,10,size = (numofnodespreviouslayer,numofnodescurrlayer))  # np.zeros(shape_wts[str(eachlayer)],dtype=np.float64) \n",
    "        \n",
    "        \n",
    "        \n",
    "        ########################                                  ############################\n",
    "        #########################                                  ##########################\n",
    "        #####################           VERY IMPORTANT!!!!!!       ########################\n",
    "        ######################                                     #######################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if (eachlayer == 0): # Any arbitrary not much to be brainstormed\n",
    "            b[str(eachlayer)] = randint(1,100,size = shape_b[str(eachlayer)]).reshape(1,cols) # np.zeros(shape_b[str(eachlayer)],dtype=np.float64) \n",
    "        else: # Shape of b is always number of nodes\n",
    "            b[str(eachlayer)] = randint(1,100,size = shape_b[str(eachlayer)]).reshape(1,numofnodescurrlayer) # np.zeros(shape_b[str(eachlayer)],dtype=np.float64) \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        ########################                                  ############################\n",
    "        #########################                                  ##########################\n",
    "        #####################           VERY IMPORTANT!!!!!!       ########################\n",
    "        ######################                                     #######################\n",
    "        \n",
    "        \n",
    "    ###### Finish initialization of weights, b for epoch 0\n",
    "    \n",
    "    #print(\"Initialized weights\",weights)\n",
    "    #print(\"Initialized b\",b)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################                            ##############\n",
    "    #################   Success Until this point! ################\n",
    "    ###################                             ###############\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_iterations = 20\n",
    "    learningratew = 1e-18\n",
    "    learningrateb = 1e-12\n",
    "    \n",
    "    Jcostfunctionold = 1000\n",
    "    \n",
    "    for eachdataset in range(len(dataset_names)):\n",
    "        \n",
    "        if(eachdataset > 0):\n",
    "            print('\\n')\n",
    "            print(\"For the dataset:{}\".format(dataset_names[eachdataset]))\n",
    "            data = features[indices_dict[eachdataset]]\n",
    "            outputlabels = diagnosis_res[indices_dict[eachdataset]]\n",
    "            [rows, cols] = np.shape(data)\n",
    "            \n",
    "            #dj_dw = [] # Same as number of weights which is inturn equal to number of features in Input data\n",
    "            #dj_db = [] # Scalar for all bs number of bs is equal to number of hidden nodes in the hidden layer\n",
    "            dj_dw = np.zeros([cols,1],dtype=np.float64) # Weights oriented as numberoffeatures(here 10) X number of nodes in current layer\n",
    "            dj_db = np.zeros(1,dtype=np.float64) # Will be a common scalar to be subtracted from all the number of b's \n",
    "                                                  # through out hidden layer, which is equal to number of nodes in hidden layer\n",
    "            \n",
    "            \n",
    "            for eachepoch in range(max_iterations):\n",
    "                #print(\"Epoch No:\",eachepoch)\n",
    "                for eachlayer in range(num_oflayers):\n",
    "                    if(eachlayer == 0):  # In Input layer input activations shall be only data\n",
    "                        inputs_activations = data\n",
    "                    elif(eachlayer == num_oflayers): # When in outputlayer here layer 2 in 3 layer ANN\n",
    "                                                         # Perform logistic as patient data is classification problem\n",
    "                        inputs_activations = logistic_reg(inputs_activations)\n",
    "                       \n",
    "                        \n",
    "                    else:\n",
    "                        #print(\"layer no:\",eachlayer)\n",
    "                        ## Since this is a single hidden layer ANN, only (Hidden) (index:layer 1)\n",
    "                        # Linear OR logistic OR softmax OR ReLU is done\n",
    "                        lin_reg_output = lin_reg(inputs_activations,weights[str(eachlayer)],b[str(eachlayer)])\n",
    "                        #print(\"\\n\")\n",
    "                        #print(lin_reg_output)\n",
    "                        #print(\"\\n\")\n",
    "                        #log_reg_output = logistic_reg(lin_reg_output)\n",
    "                        #relu_output = relu_procedure(lin_reg_output)\n",
    "                        if(eachlayer == 2): # In output layer Logistic regression applied\n",
    "                            inputs_activations = logistic_reg(lin_reg_output)\n",
    "                        else:\n",
    "                            inputs_activations = lin_reg_output\n",
    "                        #softmax_output = softmax_reg()\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                # layer 0: input layer, layer 1: hidden layer, layer 2: output layer        \n",
    "                if(num_oflayers==3):\n",
    "                    error_dataset = np.zeros(rows,dtype = np.float64)\n",
    "                    lossfunction_dataset = np.zeros(rows,dtype = np.float64)\n",
    "                    error,lossfunction = calculate_error(inputs_activations,rows,cols,outputlabels,error_dataset,lossfunction_dataset)\n",
    "                    #print(\"Output layer predictions:\",inputs_activations,\"\\n\")\n",
    "                #print(\"Error at end of {} epoch is:{}\".format(eachepoch,error),'\\n')\n",
    "                #print(\"Loss at end of {} epoch is:{}\".format(eachepoch,lossfunction),'\\n')\n",
    "                \n",
    "                dj_dw,dj_db = computedjdw(dj_dw,dj_db,error,features,rows,cols)\n",
    "                #print(\"size of dj_dw:\",np.shape(dj_dw),'\\n')\n",
    "                #print(\"size of dj_db:\",np.shape(dj_db),'\\n')\n",
    "                reg_term = 0\n",
    "                costfunction = computecostfunction(lossfunction,rows,weights[str(1)],reg_term)\n",
    "            \n",
    "            ###############                  Thumb rules                    ############\n",
    "            ###############       High Jtrain & High Jcv High bias           #############\n",
    "            ###############      Low Jtrain & High Jcv   High variance       #############\n",
    "            ################# Low Jtrain & Low Jcv    Model trained just right  ##############\n",
    "            #############                                                 #############\n",
    "            ################                                               ##############\n",
    "            \n",
    "                Jcostfunctionnew = costfunction\n",
    "                if(Jcostfunctionold<Jcostfunctionnew):\n",
    "                    #   print(\"\\n\")\n",
    "                    #    print(\"Ended at epoch no:\",eachepoch)\n",
    "                    #    print(\"\\n\")\n",
    "                    #    print(\"Final (based on logistic) predictions:\",log_reg_output)\n",
    "                    #    print(\"\\n\")\n",
    "                    break\n",
    "                else:\n",
    "                    #    print(\"Costfunction value:\",Jcostfunctionold)\n",
    "                    #    print(\"\\n\")\n",
    "                    #    print(\"(based on logistic) predictions:\",log_reg_output)\n",
    "                    #    print(\"\\n\")\n",
    "                    Jcostfunctionold = Jcostfunctionnew\n",
    "                    weights[str(1)],b[str(1)] = updatewandb(learningratew,learningrateb,weights[str(1)],b[str(1)],dj_dw,dj_db)\n",
    "                        \n",
    "            \n",
    "            \n",
    "            print(\"For dataset {}: Cost function at end of epoch no:{} is:{}\".format(dataset_names[eachdataset],eachepoch,costfunction))\n",
    "            #print(\"Outputs or Predictions are:{}\".format(inputs_activations))\n",
    "            #print('Optimized weights are:{} and optimzied bias is:{}'.format(weights[str(1)],b[str(1)]))\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1513f",
   "metadata": {},
   "source": [
    "### Concept check weight updation ######\n",
    "\n",
    "weights = randint(1,4,size=(10,23))  ### Weights matrix of size 10 X 23...\n",
    "# as Input layer has data with 10 features (previous layer nodes) and Hidden layer has 23 nodes (user given)\n",
    "\n",
    "#print(np.shape(weights))\n",
    "\n",
    "dj_dw = randint(0,2,size=(10,1))    ### dj_dw has 10 elements corresponding to 10 weights which inturn..\n",
    "#print(np.shape(dj_dw))             ## correspond to 10 features in the input data\n",
    "\n",
    "\n",
    "#print(weights)\n",
    "#print(dj_dw)\n",
    "#print(weights-dj_dw)\n",
    "\n",
    "\n",
    "biases =  randint(1,4,size=(23))\n",
    "print(np.shape(biases))\n",
    "\n",
    "dj_db = randint(1,10,size=(1))\n",
    "print(np.shape(dj_db))\n",
    "print(biases)\n",
    "print(dj_db)\n",
    "print(biases-dj_db)\n",
    "###### Passed #######"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
