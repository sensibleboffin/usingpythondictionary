{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np; # Mostly used for array manipulation datatype numpy.ndarray\n",
    "from sklearn.linear_model import LinearRegression # Used for implementing Logistic regression\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt # Plotting in python3 using Matplotlib library\n",
    "import math # math module for exp function; For e value\n",
    "# generate random integer values\n",
    "from numpy.random import seed\n",
    "from numpy.random import randint\n",
    "import array\n",
    "# seed random number generator\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_shapes(num_oflayers,nodes_eachlayer,rows,cols,eachlayer,shape_wts,shape_b):\n",
    "    for eachlayer in range(num_oflayers):\n",
    "        ## shape of weights in given current layer is nodesinpreviouslayer X nodesincurrentlayer\n",
    "        if(eachlayer == 0):\n",
    "            shape_forwt = (cols,nodes_eachlayer[1])\n",
    "            shape_wts[str(eachlayer)] = shape_forwt\n",
    "            \n",
    "            shape_forb = rows\n",
    "            shape_b[str(eachlayer)] = shape_forb \n",
    "        else:\n",
    "            shape_forwt = (nodes_eachlayer[eachlayer-1],nodes_eachlayer[eachlayer])\n",
    "            shape_wts[str(eachlayer)] = shape_forwt\n",
    "                        \n",
    "            shape_forb = rows\n",
    "            shape_b[str(eachlayer)] = shape_forb\n",
    "    \n",
    "    \n",
    "    return shape_wts,shape_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(features,weights,b):\n",
    "    lin_reg_output = np.dot(features,weights)+b\n",
    "    \n",
    "    lin_reg_output = np.clip(lin_reg_output,a_min = 1e-5, a_max = 1)\n",
    "    \n",
    "    #print(\"shape of input activations:\",np.shape(features))\n",
    "    #print(\"shape of weights:\",np.shape(weights))\n",
    "    #print(\"shape of b:\",np.shape(b))\n",
    "    #print(\"shape of lin reg output:\",np.shape(lin_reg_output))\n",
    "\n",
    "    return lin_reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5080f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg(lin_reg_output):\n",
    "    log_reg_output = 1/(1+np.exp(-lin_reg_output))\n",
    "    \n",
    "    #print(\"shape of logistic reg output:\",np.shape(log_reg_output))\n",
    "    #print(\"\\n\")\n",
    "    #print(\"log regression outputs, BEFORE clipping:\\n\")\n",
    "    #print(log_reg_output)\n",
    "    #print(\"\\n\")\n",
    "    #log_reg_output = np.clip(log_reg_output,a_min = 1e-1, a_max = 0.95)\n",
    "    #print(\"log regression outputs, AFTER clipping:\\n\")\n",
    "    #print(log_reg_output)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return log_reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_procedure(lin_reg_output):\n",
    "    #calculate ReLU g(lin_reg or z) = max(0,z) when z>=0 and g(z)=0 when z<0\n",
    "    #print(\"linear regression output:\",lin_reg_output)\n",
    "    relu_val = lin_reg_output\n",
    "    relu_val[relu_val<0]=0\n",
    "    relu_output = relu_val\n",
    "    #print(\"shape of ReLU output:\",np.shape(relu_output))\n",
    "\n",
    "    return relu_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46bfdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(inputs_activations,rows,cols,diagnosis_res,error,lossfunction):\n",
    "    # error computed from subtracting logistic output & diagnosis \n",
    "    log_reg_output = logistic_reg(inputs_activations)\n",
    "    epsilon = 0\n",
    "    for eachrow in range(rows):\n",
    "        error[eachrow] = abs(log_reg_output[eachrow] - diagnosis_res[eachrow])\n",
    "        #print(\"\\n \\n \\n \")\n",
    "        #print(\"loss function calculation:\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(diagnosis_res[eachrow] == 0):\n",
    "            #print(\"For Diagnosis 0:\")\n",
    "            #print(\"log_reg_output[eachrow]\")\n",
    "            #print(np.log(log_reg_output[eachrow]))\n",
    "            lossfunction[eachrow] = (-(diagnosis_res[eachrow])*np.log(log_reg_output[eachrow]))-(1-diagnosis_res[eachrow])*(np.log(1-log_reg_output[eachrow]+epsilon))\n",
    "        else:\n",
    "            #print(\"\\n\")\n",
    "            #print(\"For Diagnosis 1:\")\n",
    "            #print(\"log_reg_output[eachrow]\")\n",
    "            #print(np.log(log_reg_output[eachrow]))\n",
    "            lossfunction[eachrow] = (-(diagnosis_res[eachrow])*np.log(log_reg_output[eachrow]+epsilon))-(1-diagnosis_res[eachrow])*(np.log(1-log_reg_output[eachrow]))\n",
    "        \n",
    "        \n",
    "    return error,lossfunction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5808a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computedjdw(dj_dw,dj_db,error,features,rows,cols):\n",
    "    for eachrow in range(rows):\n",
    "        for eachcol in range(cols):\n",
    "            dj_dw[eachcol] = dj_dw[eachcol] + error[eachrow] + features[eachrow,eachcol]\n",
    "        dj_db[eachrow] = dj_db[eachrow] + error[eachrow]\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computecostfunction(lossfunction,rows):\n",
    "    costfunction = sum(lossfunction)/rows\n",
    "    return costfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2585bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatewandb(learningratew,learningrateb,weights,b,dj_dw,dj_db):\n",
    "    weights = weights-learningratew * dj_dw\n",
    "    b = b-learningrateb * dj_db\n",
    "    \n",
    "    return weights,b    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_learningalgo(num_oflayers,selection):\n",
    "    choice = \"N\"\n",
    "    for eachlayer in range(num_oflayers):\n",
    "        while(choice == \"N\"):\n",
    "            if(eachlayer > 0):\n",
    "                print(\"You are in layer no:\",eachlayer)\n",
    "                print(\"Enter R for multiple regression, L for logistic regression, U for ReLU, X for softmax regression\")\n",
    "                choice = str(input())\n",
    "                selection[str(eachlayer)] = choice\n",
    "                print(\"Press Y to confirm\")\n",
    "                choice = str(input())\n",
    "    return selection             \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb11672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "       \n",
    "    # Input feature 1 patient age\n",
    "    # Input feature 2 tumor size\n",
    "    # Input feature 3 family history (y/n:0/1)\n",
    "    # Input feature 4 regular junk food intake\n",
    "    # Input feature 5 currently smoking\n",
    "    # Input feature 6 stressful lifestyle\n",
    "    # Input feature 7 working with chemicals\n",
    "    # Input feature 8 urban/rural (0/1)\n",
    "    # Input feature 9 sleep duration (1hr-10hrs)\n",
    "    # Input feature 10 regular exercise\n",
    "    \n",
    "    num_ofpatients = 398\n",
    "    pat_age = randint(0,100,num_ofpatients)\n",
    "    tum_size = randint(1,10,num_ofpatients) # in cm\n",
    "    fm_histry = randint(0,2,num_ofpatients)\n",
    "    junk_food = randint(0,2,num_ofpatients)\n",
    "    smoking = randint(0,2,num_ofpatients)\n",
    "    stress_lif = randint(0,2,num_ofpatients)\n",
    "    work_withchem = randint(0,2,num_ofpatients)\n",
    "    rural_urbn = randint(0,2,num_ofpatients)\n",
    "    sleep_dur = randint(1,10,num_ofpatients)\n",
    "    reg_exerc = randint(0,2,num_ofpatients)\n",
    "    features = np.transpose([pat_age, tum_size, fm_histry, junk_food,smoking,stress_lif,work_withchem,rural_urbn,sleep_dur,reg_exerc]);\n",
    "    [rows,cols] = np.shape(features)\n",
    "    \n",
    "    # Output benign or malignant 0/1\n",
    "    diagnosis_res = randint(0,2,num_ofpatients)\n",
    "    #print(\"\\n \\n\")\n",
    "    #print(\"Diagnosis values:\")\n",
    "    #print(diagnosis_res)\n",
    "    #print(\"\\n \\n\")\n",
    "    \n",
    "        \n",
    "    print(\"Enter number of layers:\")\n",
    "    num_oflayers = int(input())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Prepare each layer with desired number of nodes ######\n",
    "    \n",
    "    nodes_eachlayer = np.empty([num_oflayers],dtype=int)\n",
    "    \n",
    "    \n",
    "    for eachlayer in range(num_oflayers):\n",
    "       \n",
    "        if(eachlayer == 0):\n",
    "            nodes_eachlayer[eachlayer] = cols\n",
    "        else:\n",
    "            print(\"Enter num of nodes for layer no:\", eachlayer)\n",
    "            nodes_eachlayer[eachlayer] = int(input())\n",
    "    #print(nodes_eachlayer)\n",
    "    \n",
    "            \n",
    "        \n",
    "    ########### Prepare each hidden layer with desired learning algorithm ############\n",
    "    #selection = {}\n",
    "    #selection = select_learningalgo(num_oflayers,selection)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########### Prepare each hidden layer with desired learning algorithm ############\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ######## Prepare shapes of weights and bias per each layer #########\n",
    "    \n",
    "    weights = {}\n",
    "    b = {}\n",
    "    shape_wts = {}\n",
    "    shape_b = {}\n",
    "    shape_wts, shape_b = prepare_shapes(num_oflayers,nodes_eachlayer,rows,cols,eachlayer,shape_wts,shape_b)\n",
    "    \n",
    "    #print(\"weight shapes dictionary\",shape_wts)\n",
    "    #print(\"b shapes dictionary\",shape_b)\n",
    "    \n",
    "    \n",
    "    ############# No ambiguity w.r.t shapes of weights and bias for each layer ###\n",
    "    \n",
    "    dj_dw = {}\n",
    "    dj_db = {}\n",
    "    error = np.zeros(rows,dtype = np.float64)\n",
    "    lossfunction = np.zeros(rows,dtype = np.float64)\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_iterations = 20\n",
    "    learningratew = 1e-13\n",
    "    learningrateb = 1e-16\n",
    "       \n",
    "    \n",
    "    #print(\"Input features dim:\",np.shape(features))\n",
    "    \n",
    "    # initialize weights and b before running ANN, which go into 1st epoch of the ANN\n",
    "    \n",
    "    for eachlayer in range(num_oflayers):\n",
    "        # [numofnodespreviouslayer,numofnodescurrlayer] = \n",
    "        numofnodespreviouslayer = shape_wts[str(eachlayer)][0]\n",
    "        numofnodescurrlayer = shape_wts[str(eachlayer)][1]\n",
    "        weights[str(eachlayer)] = randint(1,10,size = (numofnodespreviouslayer,numofnodescurrlayer))  # np.zeros(shape_wts[str(eachlayer)],dtype=np.float64) \n",
    "        \n",
    "        \n",
    "        \n",
    "        ########################                                  ############################\n",
    "        #########################                                  ##########################\n",
    "        #####################           VERY IMPORTANT!!!!!!       ########################\n",
    "        ######################                                     #######################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        b[str(eachlayer)] = randint(1,100,size = shape_b[str(eachlayer)]).reshape(rows,1) # np.zeros(shape_b[str(eachlayer)],dtype=np.float64) \n",
    "        \n",
    "        dj_dw = np.zeros([cols,1],dtype=np.float64)\n",
    "        \n",
    "        dj_db = np.zeros([shape_b[str(eachlayer)],1],dtype=np.float64)\n",
    "        \n",
    "        \n",
    "        ########################                                  ############################\n",
    "        #########################                                  ##########################\n",
    "        #####################           VERY IMPORTANT!!!!!!       ########################\n",
    "        ######################                                     #######################\n",
    "        \n",
    "        \n",
    "    ###### Finish initialization of weights, b for epoch 0\n",
    "    \n",
    "    #print(\"Initialized weights\",weights)\n",
    "    #print(\"Initialized b\",b)\n",
    "    \n",
    "    Jcostfunctionold = 1000\n",
    "    \n",
    "    for eachepoch in range(max_iterations):\n",
    "        #print(\"Epoch No:\",eachepoch)\n",
    "        for eachlayer in range(num_oflayers):\n",
    "            if(eachlayer == 0):\n",
    "                inputs_activations = features\n",
    "            \n",
    "            else:\n",
    "                #print(\"layer no:\",eachlayer)\n",
    "                ## Since this is a single hidden layer ANN, only (Hidden) (index:layer 1)\n",
    "                # Linear OR logistic OR softmax OR ReLU is done\n",
    "                lin_reg_output = lin_reg(inputs_activations,weights[str(eachlayer)],b[str(eachlayer)])\n",
    "                #print(\"\\n\")\n",
    "                #print(lin_reg_output)\n",
    "                #print(\"\\n\")\n",
    "                log_reg_output = logistic_reg(lin_reg_output)\n",
    "                relu_output = relu_procedure(lin_reg_output)\n",
    "                inputs_activations = lin_reg_output\n",
    "                #softmax_output = softmax_reg()\n",
    "    \n",
    "        if(num_oflayers==3): # layer 0: input layer, layer 1: hidden layer, layer 2: output layer\n",
    "            error = np.zeros(rows,dtype = np.float64)\n",
    "            lossfunction = np.zeros(rows,dtype = np.float64)\n",
    "            error,lossfunction = calculate_error(inputs_activations,rows,cols,diagnosis_res,error,lossfunction)\n",
    "            dj_dw,dj_db = computedjdw(dj_dw,dj_db,error,features,rows,cols)\n",
    "            costfunction = computecostfunction(lossfunction,rows)\n",
    "            \n",
    "            Jcostfunctionnew = costfunction\n",
    "            if(Jcostfunctionold<Jcostfunctionnew):\n",
    "                print(\"\\n\")\n",
    "                print(\"Ended at epoch no:\",eachepoch)\n",
    "                print(\"\\n\")\n",
    "                print(\"Final (based on logistic) predictions:\",log_reg_output)\n",
    "                print(\"\\n\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Costfunction value:\",Jcostfunctionold)\n",
    "                print(\"\\n\")\n",
    "                print(\"(based on logistic) predictions:\",log_reg_output)\n",
    "                print(\"\\n\")\n",
    "                Jcostfunctionold = Jcostfunctionnew\n",
    "                weights[str(1)],b[str(1)] = updatewandb(learningratew,learningrateb,weights[str(1)],b[str(1)],dj_dw,dj_db)\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4cac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
