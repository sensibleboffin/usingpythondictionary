{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2facbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np; # Mostly used for array manipulation datatype numpy.ndarray\n",
    "from sklearn.linear_model import LinearRegression # Used for implementing Logistic regression\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt # Plotting in python3 using Matplotlib library\n",
    "import math # math module for exp function; For e value\n",
    "# generate random integer values\n",
    "from numpy.random import seed\n",
    "from numpy.random import randint\n",
    "import array\n",
    "# seed random number generator\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd58235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_shapes(num_oflayers,nodes_eachlayer,rows,cols,eachlayer,shape_wts,shape_b):\n",
    "    for eachlayer in range(num_oflayers):\n",
    "        ## shape of weights in given current layer is nodesinpreviouslayer X nodesincurrentlayer\n",
    "        if(eachlayer == 0):\n",
    "            shape_forwt = (cols,nodes_eachlayer[1])\n",
    "            shape_wts[str(eachlayer)] = shape_forwt\n",
    "            \n",
    "            shape_forb = rows\n",
    "            shape_b[str(eachlayer)] = shape_forb \n",
    "        else:\n",
    "            shape_forwt = (nodes_eachlayer[eachlayer-1],nodes_eachlayer[eachlayer])\n",
    "            shape_wts[str(eachlayer)] = shape_forwt\n",
    "                        \n",
    "            shape_forb = rows\n",
    "            shape_b[str(eachlayer)] = shape_forb\n",
    "    \n",
    "    \n",
    "    return shape_wts,shape_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1484ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(features,weights,b):\n",
    "    lin_reg_output = np.dot(features,weights)+b\n",
    "    print(\"shape of input activations:\",np.shape(features))\n",
    "    print(\"shape of weights:\",np.shape(weights))\n",
    "    print(\"shape of b:\",np.shape(b))\n",
    "    print(\"shape of lin reg output:\",np.shape(lin_reg_output))\n",
    "\n",
    "    return lin_reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede75455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg(lin_reg_output):\n",
    "    log_reg_output = 1/(1+np.exp(-lin_reg_output))\n",
    "    print(\"shape of logistic reg output:\",np.shape(log_reg_output))\n",
    "\n",
    "    return log_reg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181f662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_procedure(lin_reg_output):\n",
    "    #calculate ReLU g(lin_reg or z) = max(0,z) when z>=0 and g(z)=0 when z<0\n",
    "    print(\"linear regression output:\",lin_reg_output)\n",
    "    relu_val = lin_reg_output\n",
    "    relu_val[relu_val<0]=0\n",
    "    relu_output = relu_val\n",
    "    print(\"shape of ReLU output:\",np.shape(relu_output))\n",
    "\n",
    "    return relu_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf07ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(inputs_activations,rows,cols,diagnosis_res,error,lossfunction):\n",
    "    # error computed from subtracting logistic output & diagnosis \n",
    "    log_reg_output = logistic_reg(inputs_activations)\n",
    "    for eachrow in range(rows):\n",
    "        error[eachrow] = abs(log_reg_output[eachrow] - diagnosis_res[eachrow])\n",
    "        lossfunction[eachrow] = (-(diagnosis_res[eachrow])*np.log(log_reg_output[eachrow]))-(1-diagnosis_res[eachrow])*(np.log(1-log_reg_output[eachrow]))\n",
    "    return error,lossfunction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff47b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computedjdw(dj_dw,dj_db,error,features,rows,cols):\n",
    "    for eachrow in range(rows):\n",
    "        for eachcol in range(cols):\n",
    "            dj_dw[eachcol] = dj_dw[eachcol] + error[eachrow] + features[eachrow,eachcol]\n",
    "        dj_db[eachrow] = dj_db[eachrow] + error[eachrow]\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "047a6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computecostfunction(lossfunction,rows):\n",
    "    costfunction = sum(lossfunction)/rows\n",
    "    return costfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ec8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of layers:\n",
      "3\n",
      "Enter num of nodes for layer no: 1\n",
      "23\n",
      "Enter num of nodes for layer no: 2\n",
      "1\n",
      "weight shapes dictionary {'0': (10, 23), '1': (10, 23), '2': (23, 1)}\n",
      "b shapes dictionary {'0': 398, '1': 398, '2': 398}\n",
      "Input features dim: (398, 10)\n",
      "Epoch No: 0\n",
      "layer no: 1\n",
      "shape of input activations: (398, 10)\n",
      "shape of weights: (10, 23)\n",
      "shape of b: (398, 1)\n",
      "shape of lin reg output: (398, 23)\n",
      "shape of logistic reg output: (398, 23)\n",
      "linear regression output: [[445 251 456 ... 181 269 477]\n",
      " [180 120 185 ...  89 147 215]\n",
      " [733 421 747 ... 223 345 769]\n",
      " ...\n",
      " [340 168 342 ...  89 140 349]\n",
      " [187 109 206 ...  71 121 213]\n",
      " [299 148 318 ...  88 153 320]]\n",
      "shape of ReLU output: (398, 23)\n",
      "layer no: 2\n",
      "shape of input activations: (398, 23)\n",
      "shape of weights: (23, 1)\n",
      "shape of b: (398, 1)\n",
      "shape of lin reg output: (398, 1)\n",
      "shape of logistic reg output: (398, 1)\n",
      "linear regression output: [[43346]\n",
      " [19717]\n",
      " [66566]\n",
      " [19935]\n",
      " [59239]\n",
      " [18923]\n",
      " [71234]\n",
      " [46900]\n",
      " [20359]\n",
      " [19515]\n",
      " [61860]\n",
      " [66635]\n",
      " [14563]\n",
      " [36896]\n",
      " [44092]\n",
      " [28353]\n",
      " [32957]\n",
      " [61776]\n",
      " [22078]\n",
      " [43991]\n",
      " [28576]\n",
      " [31199]\n",
      " [50753]\n",
      " [63854]\n",
      " [67742]\n",
      " [76024]\n",
      " [77383]\n",
      " [80030]\n",
      " [65087]\n",
      " [28264]\n",
      " [20066]\n",
      " [12000]\n",
      " [50319]\n",
      " [49477]\n",
      " [29536]\n",
      " [54814]\n",
      " [ 8244]\n",
      " [24839]\n",
      " [48963]\n",
      " [64764]\n",
      " [13769]\n",
      " [66101]\n",
      " [22974]\n",
      " [39465]\n",
      " [52062]\n",
      " [28460]\n",
      " [64380]\n",
      " [13928]\n",
      " [63962]\n",
      " [25241]\n",
      " [52295]\n",
      " [39436]\n",
      " [23502]\n",
      " [63260]\n",
      " [30330]\n",
      " [52186]\n",
      " [51505]\n",
      " [36302]\n",
      " [46372]\n",
      " [64805]\n",
      " [46958]\n",
      " [71428]\n",
      " [24838]\n",
      " [57346]\n",
      " [61594]\n",
      " [27791]\n",
      " [80155]\n",
      " [71862]\n",
      " [21395]\n",
      " [32590]\n",
      " [27828]\n",
      " [28430]\n",
      " [22014]\n",
      " [56514]\n",
      " [23928]\n",
      " [35075]\n",
      " [29132]\n",
      " [46590]\n",
      " [78270]\n",
      " [47277]\n",
      " [16300]\n",
      " [35316]\n",
      " [39538]\n",
      " [20420]\n",
      " [30880]\n",
      " [24554]\n",
      " [69508]\n",
      " [35538]\n",
      " [67346]\n",
      " [73243]\n",
      " [60532]\n",
      " [54349]\n",
      " [50194]\n",
      " [36344]\n",
      " [66437]\n",
      " [33310]\n",
      " [52797]\n",
      " [58697]\n",
      " [63530]\n",
      " [22742]\n",
      " [15207]\n",
      " [62708]\n",
      " [13498]\n",
      " [43886]\n",
      " [71899]\n",
      " [52904]\n",
      " [18601]\n",
      " [67942]\n",
      " [76718]\n",
      " [30274]\n",
      " [64615]\n",
      " [23434]\n",
      " [58133]\n",
      " [66148]\n",
      " [63065]\n",
      " [64313]\n",
      " [40793]\n",
      " [25264]\n",
      " [28565]\n",
      " [32794]\n",
      " [18673]\n",
      " [37925]\n",
      " [51521]\n",
      " [43517]\n",
      " [66665]\n",
      " [73656]\n",
      " [32153]\n",
      " [28484]\n",
      " [30931]\n",
      " [69010]\n",
      " [23167]\n",
      " [28131]\n",
      " [60282]\n",
      " [66267]\n",
      " [22906]\n",
      " [44139]\n",
      " [16790]\n",
      " [54209]\n",
      " [72619]\n",
      " [50826]\n",
      " [37677]\n",
      " [66136]\n",
      " [72806]\n",
      " [15174]\n",
      " [72860]\n",
      " [76165]\n",
      " [30267]\n",
      " [42716]\n",
      " [75453]\n",
      " [67436]\n",
      " [61703]\n",
      " [61592]\n",
      " [55201]\n",
      " [58034]\n",
      " [55447]\n",
      " [57957]\n",
      " [28683]\n",
      " [ 8822]\n",
      " [32313]\n",
      " [45432]\n",
      " [29774]\n",
      " [48654]\n",
      " [29281]\n",
      " [33518]\n",
      " [68077]\n",
      " [47990]\n",
      " [73883]\n",
      " [16653]\n",
      " [81077]\n",
      " [54833]\n",
      " [56030]\n",
      " [47329]\n",
      " [69509]\n",
      " [30526]\n",
      " [72718]\n",
      " [68981]\n",
      " [26578]\n",
      " [32280]\n",
      " [17627]\n",
      " [64730]\n",
      " [44486]\n",
      " [23849]\n",
      " [66310]\n",
      " [51745]\n",
      " [43915]\n",
      " [27384]\n",
      " [26902]\n",
      " [20912]\n",
      " [63535]\n",
      " [67918]\n",
      " [54172]\n",
      " [56783]\n",
      " [34428]\n",
      " [33012]\n",
      " [35880]\n",
      " [63704]\n",
      " [67526]\n",
      " [46893]\n",
      " [45796]\n",
      " [60056]\n",
      " [44195]\n",
      " [58872]\n",
      " [41889]\n",
      " [72644]\n",
      " [27195]\n",
      " [30402]\n",
      " [44967]\n",
      " [56077]\n",
      " [55725]\n",
      " [59036]\n",
      " [52966]\n",
      " [14923]\n",
      " [26118]\n",
      " [57301]\n",
      " [61192]\n",
      " [34679]\n",
      " [45725]\n",
      " [70508]\n",
      " [41346]\n",
      " [12532]\n",
      " [59821]\n",
      " [63923]\n",
      " [45676]\n",
      " [52165]\n",
      " [35685]\n",
      " [35561]\n",
      " [54294]\n",
      " [60538]\n",
      " [34032]\n",
      " [26216]\n",
      " [75569]\n",
      " [65035]\n",
      " [18983]\n",
      " [69447]\n",
      " [36755]\n",
      " [14334]\n",
      " [62780]\n",
      " [69569]\n",
      " [45830]\n",
      " [62230]\n",
      " [42014]\n",
      " [51833]\n",
      " [67174]\n",
      " [33613]\n",
      " [25329]\n",
      " [39260]\n",
      " [25846]\n",
      " [31173]\n",
      " [70535]\n",
      " [19887]\n",
      " [33115]\n",
      " [27117]\n",
      " [73248]\n",
      " [14348]\n",
      " [77864]\n",
      " [48971]\n",
      " [43197]\n",
      " [47817]\n",
      " [41541]\n",
      " [36005]\n",
      " [31646]\n",
      " [53990]\n",
      " [29272]\n",
      " [64336]\n",
      " [72346]\n",
      " [33032]\n",
      " [49801]\n",
      " [20905]\n",
      " [70795]\n",
      " [50160]\n",
      " [71990]\n",
      " [35544]\n",
      " [46111]\n",
      " [62961]\n",
      " [53578]\n",
      " [67033]\n",
      " [82218]\n",
      " [29021]\n",
      " [19607]\n",
      " [56518]\n",
      " [47875]\n",
      " [46369]\n",
      " [65356]\n",
      " [33558]\n",
      " [38454]\n",
      " [35177]\n",
      " [64409]\n",
      " [31093]\n",
      " [25815]\n",
      " [34724]\n",
      " [47704]\n",
      " [70932]\n",
      " [65718]\n",
      " [58877]\n",
      " [27844]\n",
      " [50954]\n",
      " [47379]\n",
      " [50438]\n",
      " [41899]\n",
      " [53055]\n",
      " [16896]\n",
      " [70820]\n",
      " [29982]\n",
      " [24169]\n",
      " [19083]\n",
      " [69324]\n",
      " [47217]\n",
      " [25038]\n",
      " [57854]\n",
      " [32515]\n",
      " [17994]\n",
      " [61498]\n",
      " [26471]\n",
      " [47107]\n",
      " [11233]\n",
      " [56664]\n",
      " [74368]\n",
      " [67745]\n",
      " [37770]\n",
      " [45221]\n",
      " [56648]\n",
      " [59361]\n",
      " [25098]\n",
      " [65518]\n",
      " [30126]\n",
      " [21495]\n",
      " [49796]\n",
      " [63466]\n",
      " [42539]\n",
      " [63651]\n",
      " [39110]\n",
      " [39661]\n",
      " [36613]\n",
      " [30839]\n",
      " [81205]\n",
      " [45132]\n",
      " [60916]\n",
      " [27093]\n",
      " [51237]\n",
      " [32392]\n",
      " [61854]\n",
      " [55948]\n",
      " [78620]\n",
      " [47046]\n",
      " [36638]\n",
      " [60632]\n",
      " [45847]\n",
      " [66220]\n",
      " [60248]\n",
      " [77665]\n",
      " [32411]\n",
      " [34246]\n",
      " [36049]\n",
      " [56412]\n",
      " [63617]\n",
      " [67586]\n",
      " [43682]\n",
      " [70975]\n",
      " [39777]\n",
      " [48843]\n",
      " [15579]\n",
      " [66949]\n",
      " [79394]\n",
      " [72844]\n",
      " [26808]\n",
      " [70535]\n",
      " [33286]\n",
      " [50278]\n",
      " [36892]\n",
      " [76067]\n",
      " [46325]\n",
      " [11799]\n",
      " [56600]\n",
      " [27173]\n",
      " [48086]\n",
      " [59751]\n",
      " [43478]\n",
      " [46435]\n",
      " [59542]\n",
      " [23858]\n",
      " [36327]\n",
      " [23000]\n",
      " [26187]\n",
      " [60854]\n",
      " [74319]\n",
      " [75417]\n",
      " [48006]\n",
      " [18874]\n",
      " [53033]\n",
      " [53945]\n",
      " [66955]\n",
      " [78760]\n",
      " [60458]\n",
      " [63391]\n",
      " [25499]\n",
      " [29227]\n",
      " [18836]\n",
      " [27423]]\n",
      "shape of ReLU output: (398, 1)\n",
      "shape of logistic reg output: (398, 1)\n",
      "Costfunction value: 1000\n",
      "Epoch No: 1\n",
      "layer no: 1\n",
      "shape of input activations: (398, 10)\n",
      "shape of weights: (10, 23)\n",
      "shape of b: (398, 1)\n",
      "shape of lin reg output: (398, 23)\n",
      "shape of logistic reg output: (398, 23)\n",
      "linear regression output: [[445 251 456 ... 181 269 477]\n",
      " [180 120 185 ...  89 147 215]\n",
      " [733 421 747 ... 223 345 769]\n",
      " ...\n",
      " [340 168 342 ...  89 140 349]\n",
      " [187 109 206 ...  71 121 213]\n",
      " [299 148 318 ...  88 153 320]]\n",
      "shape of ReLU output: (398, 23)\n",
      "layer no: 2\n",
      "shape of input activations: (398, 23)\n",
      "shape of weights: (23, 1)\n",
      "shape of b: (398, 1)\n",
      "shape of lin reg output: (398, 1)\n",
      "shape of logistic reg output: (398, 1)\n",
      "linear regression output: [[43346]\n",
      " [19717]\n",
      " [66566]\n",
      " [19935]\n",
      " [59239]\n",
      " [18923]\n",
      " [71234]\n",
      " [46900]\n",
      " [20359]\n",
      " [19515]\n",
      " [61860]\n",
      " [66635]\n",
      " [14563]\n",
      " [36896]\n",
      " [44092]\n",
      " [28353]\n",
      " [32957]\n",
      " [61776]\n",
      " [22078]\n",
      " [43991]\n",
      " [28576]\n",
      " [31199]\n",
      " [50753]\n",
      " [63854]\n",
      " [67742]\n",
      " [76024]\n",
      " [77383]\n",
      " [80030]\n",
      " [65087]\n",
      " [28264]\n",
      " [20066]\n",
      " [12000]\n",
      " [50319]\n",
      " [49477]\n",
      " [29536]\n",
      " [54814]\n",
      " [ 8244]\n",
      " [24839]\n",
      " [48963]\n",
      " [64764]\n",
      " [13769]\n",
      " [66101]\n",
      " [22974]\n",
      " [39465]\n",
      " [52062]\n",
      " [28460]\n",
      " [64380]\n",
      " [13928]\n",
      " [63962]\n",
      " [25241]\n",
      " [52295]\n",
      " [39436]\n",
      " [23502]\n",
      " [63260]\n",
      " [30330]\n",
      " [52186]\n",
      " [51505]\n",
      " [36302]\n",
      " [46372]\n",
      " [64805]\n",
      " [46958]\n",
      " [71428]\n",
      " [24838]\n",
      " [57346]\n",
      " [61594]\n",
      " [27791]\n",
      " [80155]\n",
      " [71862]\n",
      " [21395]\n",
      " [32590]\n",
      " [27828]\n",
      " [28430]\n",
      " [22014]\n",
      " [56514]\n",
      " [23928]\n",
      " [35075]\n",
      " [29132]\n",
      " [46590]\n",
      " [78270]\n",
      " [47277]\n",
      " [16300]\n",
      " [35316]\n",
      " [39538]\n",
      " [20420]\n",
      " [30880]\n",
      " [24554]\n",
      " [69508]\n",
      " [35538]\n",
      " [67346]\n",
      " [73243]\n",
      " [60532]\n",
      " [54349]\n",
      " [50194]\n",
      " [36344]\n",
      " [66437]\n",
      " [33310]\n",
      " [52797]\n",
      " [58697]\n",
      " [63530]\n",
      " [22742]\n",
      " [15207]\n",
      " [62708]\n",
      " [13498]\n",
      " [43886]\n",
      " [71899]\n",
      " [52904]\n",
      " [18601]\n",
      " [67942]\n",
      " [76718]\n",
      " [30274]\n",
      " [64615]\n",
      " [23434]\n",
      " [58133]\n",
      " [66148]\n",
      " [63065]\n",
      " [64313]\n",
      " [40793]\n",
      " [25264]\n",
      " [28565]\n",
      " [32794]\n",
      " [18673]\n",
      " [37925]\n",
      " [51521]\n",
      " [43517]\n",
      " [66665]\n",
      " [73656]\n",
      " [32153]\n",
      " [28484]\n",
      " [30931]\n",
      " [69010]\n",
      " [23167]\n",
      " [28131]\n",
      " [60282]\n",
      " [66267]\n",
      " [22906]\n",
      " [44139]\n",
      " [16790]\n",
      " [54209]\n",
      " [72619]\n",
      " [50826]\n",
      " [37677]\n",
      " [66136]\n",
      " [72806]\n",
      " [15174]\n",
      " [72860]\n",
      " [76165]\n",
      " [30267]\n",
      " [42716]\n",
      " [75453]\n",
      " [67436]\n",
      " [61703]\n",
      " [61592]\n",
      " [55201]\n",
      " [58034]\n",
      " [55447]\n",
      " [57957]\n",
      " [28683]\n",
      " [ 8822]\n",
      " [32313]\n",
      " [45432]\n",
      " [29774]\n",
      " [48654]\n",
      " [29281]\n",
      " [33518]\n",
      " [68077]\n",
      " [47990]\n",
      " [73883]\n",
      " [16653]\n",
      " [81077]\n",
      " [54833]\n",
      " [56030]\n",
      " [47329]\n",
      " [69509]\n",
      " [30526]\n",
      " [72718]\n",
      " [68981]\n",
      " [26578]\n",
      " [32280]\n",
      " [17627]\n",
      " [64730]\n",
      " [44486]\n",
      " [23849]\n",
      " [66310]\n",
      " [51745]\n",
      " [43915]\n",
      " [27384]\n",
      " [26902]\n",
      " [20912]\n",
      " [63535]\n",
      " [67918]\n",
      " [54172]\n",
      " [56783]\n",
      " [34428]\n",
      " [33012]\n",
      " [35880]\n",
      " [63704]\n",
      " [67526]\n",
      " [46893]\n",
      " [45796]\n",
      " [60056]\n",
      " [44195]\n",
      " [58872]\n",
      " [41889]\n",
      " [72644]\n",
      " [27195]\n",
      " [30402]\n",
      " [44967]\n",
      " [56077]\n",
      " [55725]\n",
      " [59036]\n",
      " [52966]\n",
      " [14923]\n",
      " [26118]\n",
      " [57301]\n",
      " [61192]\n",
      " [34679]\n",
      " [45725]\n",
      " [70508]\n",
      " [41346]\n",
      " [12532]\n",
      " [59821]\n",
      " [63923]\n",
      " [45676]\n",
      " [52165]\n",
      " [35685]\n",
      " [35561]\n",
      " [54294]\n",
      " [60538]\n",
      " [34032]\n",
      " [26216]\n",
      " [75569]\n",
      " [65035]\n",
      " [18983]\n",
      " [69447]\n",
      " [36755]\n",
      " [14334]\n",
      " [62780]\n",
      " [69569]\n",
      " [45830]\n",
      " [62230]\n",
      " [42014]\n",
      " [51833]\n",
      " [67174]\n",
      " [33613]\n",
      " [25329]\n",
      " [39260]\n",
      " [25846]\n",
      " [31173]\n",
      " [70535]\n",
      " [19887]\n",
      " [33115]\n",
      " [27117]\n",
      " [73248]\n",
      " [14348]\n",
      " [77864]\n",
      " [48971]\n",
      " [43197]\n",
      " [47817]\n",
      " [41541]\n",
      " [36005]\n",
      " [31646]\n",
      " [53990]\n",
      " [29272]\n",
      " [64336]\n",
      " [72346]\n",
      " [33032]\n",
      " [49801]\n",
      " [20905]\n",
      " [70795]\n",
      " [50160]\n",
      " [71990]\n",
      " [35544]\n",
      " [46111]\n",
      " [62961]\n",
      " [53578]\n",
      " [67033]\n",
      " [82218]\n",
      " [29021]\n",
      " [19607]\n",
      " [56518]\n",
      " [47875]\n",
      " [46369]\n",
      " [65356]\n",
      " [33558]\n",
      " [38454]\n",
      " [35177]\n",
      " [64409]\n",
      " [31093]\n",
      " [25815]\n",
      " [34724]\n",
      " [47704]\n",
      " [70932]\n",
      " [65718]\n",
      " [58877]\n",
      " [27844]\n",
      " [50954]\n",
      " [47379]\n",
      " [50438]\n",
      " [41899]\n",
      " [53055]\n",
      " [16896]\n",
      " [70820]\n",
      " [29982]\n",
      " [24169]\n",
      " [19083]\n",
      " [69324]\n",
      " [47217]\n",
      " [25038]\n",
      " [57854]\n",
      " [32515]\n",
      " [17994]\n",
      " [61498]\n",
      " [26471]\n",
      " [47107]\n",
      " [11233]\n",
      " [56664]\n",
      " [74368]\n",
      " [67745]\n",
      " [37770]\n",
      " [45221]\n",
      " [56648]\n",
      " [59361]\n",
      " [25098]\n",
      " [65518]\n",
      " [30126]\n",
      " [21495]\n",
      " [49796]\n",
      " [63466]\n",
      " [42539]\n",
      " [63651]\n",
      " [39110]\n",
      " [39661]\n",
      " [36613]\n",
      " [30839]\n",
      " [81205]\n",
      " [45132]\n",
      " [60916]\n",
      " [27093]\n",
      " [51237]\n",
      " [32392]\n",
      " [61854]\n",
      " [55948]\n",
      " [78620]\n",
      " [47046]\n",
      " [36638]\n",
      " [60632]\n",
      " [45847]\n",
      " [66220]\n",
      " [60248]\n",
      " [77665]\n",
      " [32411]\n",
      " [34246]\n",
      " [36049]\n",
      " [56412]\n",
      " [63617]\n",
      " [67586]\n",
      " [43682]\n",
      " [70975]\n",
      " [39777]\n",
      " [48843]\n",
      " [15579]\n",
      " [66949]\n",
      " [79394]\n",
      " [72844]\n",
      " [26808]\n",
      " [70535]\n",
      " [33286]\n",
      " [50278]\n",
      " [36892]\n",
      " [76067]\n",
      " [46325]\n",
      " [11799]\n",
      " [56600]\n",
      " [27173]\n",
      " [48086]\n",
      " [59751]\n",
      " [43478]\n",
      " [46435]\n",
      " [59542]\n",
      " [23858]\n",
      " [36327]\n",
      " [23000]\n",
      " [26187]\n",
      " [60854]\n",
      " [74319]\n",
      " [75417]\n",
      " [48006]\n",
      " [18874]\n",
      " [53033]\n",
      " [53945]\n",
      " [66955]\n",
      " [78760]\n",
      " [60458]\n",
      " [63391]\n",
      " [25499]\n",
      " [29227]\n",
      " [18836]\n",
      " [27423]]\n",
      "shape of ReLU output: (398, 1)\n",
      "shape of logistic reg output: (398, 1)\n",
      "Costfunction value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27537/461491038.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  lossfunction[eachrow] = (-(diagnosis_res[eachrow])*np.log(log_reg_output[eachrow]))-(1-diagnosis_res[eachrow])*(np.log(1-log_reg_output[eachrow]))\n",
      "/tmp/ipykernel_27537/461491038.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  lossfunction[eachrow] = (-(diagnosis_res[eachrow])*np.log(log_reg_output[eachrow]))-(1-diagnosis_res[eachrow])*(np.log(1-log_reg_output[eachrow]))\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "       \n",
    "    # Input feature 1 patient age\n",
    "    # Input feature 2 tumor size\n",
    "    # Input feature 3 family history (y/n:0/1)\n",
    "    # Input feature 4 regular junk food intake\n",
    "    # Input feature 5 currently smoking\n",
    "    # Input feature 6 stressful lifestyle\n",
    "    # Input feature 7 working with chemicals\n",
    "    # Input feature 8 urban/rural (0/1)\n",
    "    # Input feature 9 sleep duration (1hr-10hrs)\n",
    "    # Input feature 10 regular exercise\n",
    "    \n",
    "    num_ofpatients = 398\n",
    "    pat_age = randint(0,100,num_ofpatients)\n",
    "    tum_size = randint(1,10,num_ofpatients) # in cm\n",
    "    fm_histry = randint(0,2,num_ofpatients)\n",
    "    junk_food = randint(0,2,num_ofpatients)\n",
    "    smoking = randint(0,2,num_ofpatients)\n",
    "    stress_lif = randint(0,2,num_ofpatients)\n",
    "    work_withchem = randint(0,2,num_ofpatients)\n",
    "    rural_urbn = randint(0,2,num_ofpatients)\n",
    "    sleep_dur = randint(1,10,num_ofpatients)\n",
    "    reg_exerc = randint(0,2,num_ofpatients)\n",
    "    features = np.transpose([pat_age, tum_size, fm_histry, junk_food,smoking,stress_lif,work_withchem,rural_urbn,sleep_dur,reg_exerc]);\n",
    "    [rows,cols] = np.shape(features)\n",
    "    \n",
    "    # Output benign or malignant 0/1\n",
    "    diagnosis_res = randint(0,2,num_ofpatients)\n",
    "    \n",
    "    \n",
    "        \n",
    "    print(\"Enter number of layers:\")\n",
    "    num_oflayers = int(input())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Prepare each layer with desired number of nodes ######\n",
    "    \n",
    "    nodes_eachlayer = np.empty([num_oflayers],dtype=int)\n",
    "    \n",
    "    \n",
    "    for eachlayer in range(num_oflayers):\n",
    "       \n",
    "        if(eachlayer == 0):\n",
    "            nodes_eachlayer[eachlayer] = cols\n",
    "        else:\n",
    "            print(\"Enter num of nodes for layer no:\", eachlayer)\n",
    "            nodes_eachlayer[eachlayer] = int(input())\n",
    "    #print(nodes_eachlayer)\n",
    "    \n",
    "    \n",
    "        \n",
    "    ######## Prepare shapes of weights and bias per each layer #########\n",
    "    weights = {}\n",
    "    b = {}\n",
    "    shape_wts = {}\n",
    "    shape_b = {}\n",
    "    shape_wts, shape_b = prepare_shapes(num_oflayers,nodes_eachlayer,rows,cols,eachlayer,shape_wts,shape_b)\n",
    "    \n",
    "    print(\"weight shapes dictionary\",shape_wts)\n",
    "    print(\"b shapes dictionary\",shape_b)\n",
    "    \n",
    "    \n",
    "    ############# No ambiguity w.r.t shapes of weights and bias for each layer ###\n",
    "    \n",
    "    dj_dw = {}\n",
    "    dj_db = {}\n",
    "    error = np.zeros(rows,dtype = np.float64)\n",
    "    lossfunction = np.zeros(rows,dtype = np.float64)\n",
    "    Jcostfunctionold = 0.00\n",
    "    \n",
    "    \n",
    "    max_iterations = 2\n",
    "    learningratew = 1e-4\n",
    "    learningrateb = 1e-7\n",
    "       \n",
    "    \n",
    "    print(\"Input features dim:\",np.shape(features))\n",
    "    \n",
    "    # initialize weights and b before running ANN, which go into 1st epoch of the ANN\n",
    "    \n",
    "    for eachlayer in range(num_oflayers):\n",
    "        # [numofnodespreviouslayer,numofnodescurrlayer] = \n",
    "        numofnodespreviouslayer = shape_wts[str(eachlayer)][0]\n",
    "        numofnodescurrlayer = shape_wts[str(eachlayer)][1]\n",
    "        weights[str(eachlayer)] = randint(1,10,size = (numofnodespreviouslayer,numofnodescurrlayer))  # np.zeros(shape_wts[str(eachlayer)],dtype=np.float64) \n",
    "        b[str(eachlayer)] = randint(1,100,size = shape_b[str(eachlayer)]).reshape(rows,1) # np.zeros(shape_b[str(eachlayer)],dtype=np.float64) \n",
    "        \n",
    "        dj_dw = np.zeros(cols,dtype=np.float64)\n",
    "        dj_db = np.zeros(shape_b[str(eachlayer)],dtype=np.float64)\n",
    "        \n",
    "        \n",
    "    ###### Finish initialization of weights, b for epoch 0\n",
    "    \n",
    "    #print(\"Initialized weights\",weights)\n",
    "    #print(\"Initialized b\",b)\n",
    "    \n",
    "    Jcostfunctionold = 1000\n",
    "    \n",
    "    for eachepoch in range(max_iterations):\n",
    "        print(\"Epoch No:\",eachepoch)\n",
    "        for eachlayer in range(num_oflayers):\n",
    "            if(eachlayer == 0):\n",
    "                inputs_activations = features\n",
    "            \n",
    "            else:\n",
    "                print(\"layer no:\",eachlayer)\n",
    "                ## Since this is a single hidden layer ANN, only (Hidden) (index:layer 1)\n",
    "                # Linear OR logistic OR softmax OR ReLU is done\n",
    "                lin_reg_output = lin_reg(inputs_activations,weights[str(eachlayer)],b[str(eachlayer)])\n",
    "                log_reg_output = logistic_reg(lin_reg_output)\n",
    "                relu_output = relu_procedure(lin_reg_output)\n",
    "                inputs_activations = lin_reg_output\n",
    "                #softmax_output = softmax_reg()\n",
    "    \n",
    "        if(num_oflayers==3): # layer 0: input layer, layer 1: hidden layer, layer 2: output layer\n",
    "            error = np.zeros(rows,dtype = float)\n",
    "            lossfunction = np.zeros(rows,dtype = float)\n",
    "            error,lossfunction = calculate_error(inputs_activations,rows,cols,diagnosis_res,error,lossfunction)\n",
    "            dj_dw,dj_db = computedjdw(dj_dw,dj_db,error,features,rows,cols)\n",
    "            costfunction = computecostfunction(lossfunction,rows)\n",
    "            \n",
    "            Jcostfunctionnew = costfunction\n",
    "            if(Jcostfunctionold<Jcostfunctionnew):\n",
    "                break\n",
    "            else:\n",
    "                print(\"Costfunction value:\",Jcostfunctionold)\n",
    "                Jcostfunctionold = Jcostfunctionnew\n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6fd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
